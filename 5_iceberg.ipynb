{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persist aggregated history to Iceberg\n",
    "\n",
    "In the real-time stage, we built low-latency features from streaming events. That is good for online risk scoring, but training needs durable history that we can recompute consistently.\n",
    "\n",
    "In this lab design, reconstructing view-related history from PostgreSQL is effectively impossible, because we do not store every raw view event there. PostgreSQL keeps current application state (for example total view counters), not complete per-view event history.\n",
    "\n",
    "For comments and flags, retrospective counting from PostgreSQL is possible in principle. But we still prefer not to depend on that for training, because repeated time-window queries over large tables are expensive and can drift from the online feature logic.\n",
    "\n",
    "This creates a model-training problem: we want feature values at arbitrary anchor times after upload (for example at upload time, +5 minutes, +30 minutes), but current-state tables alone do not reliably provide that.\n",
    "\n",
    "So in this stage, we persist aggregated event history to Iceberg tables. We will materialize window aggregates at 5-minute intervals and use those durable aggregates as a stable source for training features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Start batch workflow services (Airflow)\n",
    "\n",
    "We will use Airflow to orchestrate batch jobs that write to Iceberg. Bring up Airflow, along with Nimtable for viewing Iceberg tables:\n",
    "\n",
    "``` bash\n",
    "# run on node-data\n",
    "docker compose -f /home/cc/data-platform-chi/docker/docker-compose.yaml up -d \\\n",
    "  airflow-init airflow-webserver airflow-scheduler nimtable nimtable-web\n",
    "```\n",
    "\n",
    "The first startup can take a while. We use Airflow to orchestrate data workflows, and the Airflow image must include libraries for Iceberg and storage/database access (for example PyIceberg, Arrow, and related clients). That setup happens during the container image build, so initial bring-up is slower than the earlier services.\n",
    "\n",
    "Open Airflow:\n",
    "\n",
    "-   `http://A.B.C.D:8080`\n",
    "-   username: `admin`\n",
    "-   password: `gourmetgram_airflow`\n",
    "\n",
    "After login:\n",
    "\n",
    "1.  Click DAGs in the top navigation if you are not already on the DAG list page.\n",
    "2.  Confirm you can see both pipeline DAGs:\n",
    "\n",
    "-   `redpanda_event_aggregation`\n",
    "-   `moderation_training`\n",
    "\n",
    "At this point they may appear paused (toggle off). That is expected before we trigger runs in the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run event aggregation DAG first\n",
    "\n",
    "The `redpanda_event_aggregation` DAG reads `views`, `comments`, and `flags` events from Redpanda, groups them into 5-minute windows, and writes those window counts to Iceberg tables in `event_aggregations`.\n",
    "\n",
    "By default, this DAG is scheduled hourly (every hour on the hour). In production we would normally let the scheduler run it at those boundaries, but in this lab we trigger it early so we can see the full flow immediately.\n",
    "\n",
    "This DAG also keeps a watermark in an Airflow Variable (`redpanda_event_aggregation_checkpoint`) using Kafka offsets per topic partition. On the first run, if that Variable does not exist yet, the DAG performs a historical backfill from earliest available offsets. After a successful run, it saves the next offsets as the new watermark so later runs continue incrementally.\n",
    "\n",
    "In Airflow UI:\n",
    "\n",
    "1.  Open DAG `redpanda_event_aggregation`\n",
    "2.  Trigger a run by clicking the ▶\n",
    "3.  Open the DAG run (for example from Recent Tasks/Graph) and wait until tasks are green (success).\n",
    "\n",
    "If this is your first time running a DAG in Airflow, inspect it step by step:\n",
    "\n",
    "1.  Open the DAG, then click Graph to see task order and dependencies.\n",
    "2.  Click one task box (for example `consume_and_aggregate_events`).\n",
    "3.  In the task popup, click Logs to open that task’s execution log.\n",
    "4.  Repeat for each task (`write_view_windows`, `write_comment_windows`, `write_flag_windows`, `commit_checkpoint`) so you can see what each step read/wrote.\n",
    "5.  Use Grid view to quickly check retries, durations, and final status across all tasks in the run.\n",
    "\n",
    "This DAG is structured as:\n",
    "\n",
    "``` python\n",
    "dag = DAG(\n",
    "    'redpanda_event_aggregation',\n",
    "    schedule_interval=timedelta(hours=1),\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "t1_consume >> t2_write_views >> t3_write_comments >> t4_write_flags >> t5_commit_checkpoint\n",
    "```\n",
    "\n",
    "Watermark/backfill logic in this DAG is:\n",
    "\n",
    "``` python\n",
    "CHECKPOINT_VAR = \"redpanda_event_aggregation_checkpoint\"\n",
    "\n",
    "checkpoint = Variable.get(CHECKPOINT_VAR, default_var=None, deserialize_json=True)\n",
    "if checkpoint is None:\n",
    "    # First run: historical backfill\n",
    "    start_offsets = consumer.beginning_offsets(partitions)\n",
    "else:\n",
    "    # Incremental runs: resume from saved watermark\n",
    "    start_offsets = checkpoint[\"offsets\"]\n",
    "\n",
    "# Freeze a run boundary so this run is deterministic\n",
    "end_offsets = consumer.end_offsets(partitions)\n",
    "\n",
    "# Consume [start_offset, end_offset) for each topic partition\n",
    "# ...aggregate into 5-minute windows...\n",
    "\n",
    "# Only after all Iceberg writes succeed:\n",
    "Variable.set(CHECKPOINT_VAR, {\"offsets\": next_offsets}, serialize_json=True)\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "-   `t1_consume`: reads `views/comments/flags` events from the current watermark up to this run boundary and bins them into 5-minute windows.\n",
    "-   `t2_write_views`, `t3_write_comments`, `t4_write_flags`: write those windowed aggregates to Iceberg tables.\n",
    "-   `t5_commit_checkpoint`: saves the next Kafka offsets to the Airflow Variable watermark after all writes succeed.\n",
    "\n",
    "This DAG reads streaming events and writes 5-minute aggregate windows to Iceberg tables.\n",
    "\n",
    "After this DAG succeeds, we should be able to confirm new Iceberg window tables:\n",
    "\n",
    "-   `event_aggregations.view_windows_5min`\n",
    "-   `event_aggregations.comment_windows_5min`\n",
    "-   `event_aggregations.flag_windows_5min`\n",
    "\n",
    "Check these in Nimtable: Open `http://A.B.C.D:3000` and log in (`admin` / `gourmetgram_nimtable`).\n",
    "\n",
    "In Nimtable, connect the Iceberg catalog, which is hosted in PostgreSQL:\n",
    "\n",
    "1.  From the left sidebar, click Catalogs.\n",
    "2.  Click Add Catalog -\\> Connect Catalog.\n",
    "3.  Fill in:\n",
    "    -   Connection Method: `JDBC + S3`\n",
    "    -   Catalog Name: `gourmetgram`\n",
    "    -   Catalog Endpoint: `jdbc:postgresql://postgres:5432/iceberg_catalog`\n",
    "    -   Warehouse: `s3://gourmetgram-datalake/warehouse`\n",
    "    -   S3 Endpoint: `http://minio:9000`\n",
    "    -   Access Key: `admin`\n",
    "    -   Secret Key: `gourmetgram_minio`\n",
    "4.  Open Advanced Options and add:\n",
    "    -   `jdbc.user` = `user`\n",
    "    -   `jdbc.password` = `gourmetgram_postgres`\n",
    "5.  Click Connect Catalog.\n",
    "\n",
    "Then,\n",
    "\n",
    "1.  Open Catalogs -\\> `gourmetgram`.\n",
    "2.  Open namespace `event_aggregations`.\n",
    "3.  Confirm the three `*_windows_5min` tables are visible.\n",
    "\n",
    "Take screenshots for later reference.\n",
    "\n",
    "We are just browsing a catalog of data that is in the lakehouse, but the underlying storage is in MinIO. To see the underlying storage right after this Redpanda aggregation run:\n",
    "\n",
    "1.  Open MinIO Console at `http://A.B.C.D:9001` and log in (`admin` / `gourmetgram_minio`).\n",
    "2.  Click Object Browser, then open bucket `gourmetgram-datalake`.\n",
    "3.  Open folder `warehouse/event_aggregations/` and then one table folder such as `view_windows_5min/`.\n",
    "4.  Open `data/` to see Parquet files that store the data.\n",
    "5.  Open `metadata/` to see Iceberg metadata files (schemas, snapshots, file manifests).\n",
    "\n",
    "Take screenshots for later reference.\n",
    "\n",
    "Also inspect the watermark Variable in PostgreSQL via Adminer:\n",
    "\n",
    "1.  Open Adminer at `http://A.B.C.D:5050`.\n",
    "2.  Login with:\n",
    "    -   System: `PostgreSQL`\n",
    "    -   Server: `postgres`\n",
    "    -   Username: `user`\n",
    "    -   Password: `gourmetgram_postgres`\n",
    "    -   Database: `airflow`\n",
    "3.  Open table `variable`.\n",
    "4.  Click Select data.\n",
    "5.  Add filter `key = redpanda_event_aggregation_checkpoint`.\n",
    "6.  Click Select.\n",
    "\n",
    "How to interpret the row:\n",
    "\n",
    "-   `id`: internal primary key for this Airflow Variable row.\n",
    "-   `key`: Variable name (`redpanda_event_aggregation_checkpoint`).\n",
    "-   `val`: stored Variable value.\n",
    "-   `description`: optional note (often `NULL`).\n",
    "-   `is_encrypted`: whether Airflow encrypted `val` with its Fernet key.\n",
    "\n",
    "Assuming `is_encrypted` is `1`, so `val` appears as encrypted text in Adminer. To view the decoded JSON watermark payload (per-topic-partition offsets), query through Airflow itself:\n",
    "\n",
    "``` bash\n",
    "# run on node-data\n",
    "docker exec airflow-webserver airflow variables get redpanda_event_aggregation_checkpoint\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Run main training-data DAG\n",
    "\n",
    "The `moderation_training` DAG builds the model-training dataset. It reads application data from PostgreSQL, joins in the 5-minute Iceberg aggregates we just produced, computes training features at multiple decision points, and writes the final dataset to Iceberg (`moderation.training_data`).\n",
    "\n",
    "This DAG is scheduled daily. In normal operation we would let the daily schedule handle it, but in this lab we trigger it now so we can validate the full batch path end to end.\n",
    "\n",
    "Its task flow is:\n",
    "\n",
    "``` python\n",
    "t1_extract_data >> t2_transform_features >> t3_load_iceberg\n",
    "```\n",
    "\n",
    "Here:\n",
    "\n",
    "-   `t1_extract_data`: snapshots `users`, `images`, `comments`, and `flags` from PostgreSQL into MinIO raw parquet paths.\n",
    "-   `t2_transform_features`: applies governance filters, combines application snapshots with Iceberg window tables (`event_aggregations.*_windows_5min`), and computes training features and labels.\n",
    "-   `t3_load_iceberg`: writes the resulting training dataset to Iceberg table `moderation.training_data`.\n",
    "\n",
    "In `t2_transform_features`, we also enforce training data policy by only selecting eligible candidates for training. For example:\n",
    "\n",
    "``` python\n",
    "# examples from the DAG logic\n",
    "# exclude test accounts\n",
    "uploader_is_test = images['is_test_account'].fillna(False)\n",
    "# exclude content from minors\n",
    "uploader_is_child = images['year_of_birth'].notna() & (\n",
    "    (images['created_at'].dt.year - images['year_of_birth']) < 18\n",
    ")\n",
    "# exclude deleted content\n",
    "image_is_deleted = images['deleted_at'].notna()\n",
    "\n",
    "eligible_images = images[(~uploader_is_test) & (~uploader_is_child) & (~image_is_deleted)]\n",
    "```\n",
    "\n",
    "We apply similar policy filters for comment/flag data, then create labels and features from the filtered data.\n",
    "\n",
    "> Although it is not implemented in this lab, we should think about what would happen if content is deleted after it already entered `moderation.training_data`. We do not need a full table rebuild: Iceberg supports row-level correction workflows through snapshot updates: the pipeline can maintain exclusion keys (for example deleted `image_id` values), then execute an Iceberg `DELETE` or equivalent rewrite that removes matching rows in a new snapshot. Training readers use the latest snapshot, so excluded rows are no longer visible in current scans while table history is still preserved.\n",
    "\n",
    "In Airflow UI:\n",
    "\n",
    "1.  Open DAG `moderation_training`\n",
    "2.  Trigger a run\n",
    "3.  Open the run and wait until tasks are green (success).\n",
    "\n",
    "This builds the training dataset by combining application data and windowed aggregates, then writes output to Iceberg.\n",
    "\n",
    "After this DAG succeeds, verify the training table in Nimtable and MinIO.\n",
    "\n",
    "In Nimtable:\n",
    "\n",
    "1.  Open `http://A.B.C.D:3000` and log in (`admin` / `gourmetgram_nimtable`).\n",
    "2.  Open Catalogs -\\> `gourmetgram`.\n",
    "3.  Open namespace `moderation`.\n",
    "4.  Open table `training_data`.\n",
    "5.  Confirm schema and preview rows are visible.\n",
    "\n",
    "In MinIO:\n",
    "\n",
    "1.  Open `http://A.B.C.D:9001` and log in (`admin` / `gourmetgram_minio`).\n",
    "2.  Open bucket `gourmetgram-datalake` -\\> `warehouse/moderation/training_data/`.\n",
    "3.  Confirm both `data/` and `metadata/` folders contain files.\n",
    "4.  Optionally compare timestamps to confirm new files were written by this run."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
