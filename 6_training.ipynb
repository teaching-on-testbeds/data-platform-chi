{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a moderation model from Iceberg\n",
    "\n",
    "In the previous stage, we created `moderation.training_data` as an Iceberg table. Now can we use that table to train a model. An inference service with this model could potentially replace the heuristic “risk scoring service” that we introduced in the real time data pipeline.\n",
    "\n",
    "We are going to train a classifier in Python with scikit-learn: we will read Iceberg tables through PyIceberg, then trains a logistic regression incrementally using an `SGDClassifier`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this on `node-data`:\n",
    "\n",
    "``` bash\n",
    "# run on node-data\n",
    "docker compose -f /home/cc/data-platform-chi/docker/docker-compose.yaml up -d --build jupyter\n",
    "```\n",
    "\n",
    "The `jupyter` service in compose is:\n",
    "\n",
    "``` yaml\n",
    "jupyter:\n",
    "  build:\n",
    "    context: ../model_training\n",
    "    dockerfile: Dockerfile\n",
    "  container_name: jupyter\n",
    "  ports:\n",
    "    - \"8888:8888\"\n",
    "  environment:\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__TYPE=sql\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__URI=postgresql+psycopg2://user:gourmetgram_postgres@postgres:5432/iceberg_catalog\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__S3__ENDPOINT=http://minio:9000\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__S3__ACCESS_KEY_ID=admin\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__S3__SECRET_ACCESS_KEY=gourmetgram_minio\n",
    "    - PYICEBERG_CATALOG__GOURMETGRAM__WAREHOUSE=s3://gourmetgram-datalake/warehouse\n",
    "  volumes:\n",
    "    - ../model_training/workspace:/home/jovyan/work\n",
    "```\n",
    "\n",
    "Here,\n",
    "\n",
    "-   `build` uses the Jupyter image from `model_training/Dockerfile`.\n",
    "-   `ports` exposes notebook UI on `8888`.\n",
    "-   `PYICEBERG_CATALOG__...` points PyIceberg at the SQL catalog in PostgreSQL and warehouse files in MinIO.\n",
    "-   `volumes` mounts `model_training/workspace` into `/home/jovyan/work`, so notebooks and saved artifacts persist in the lab directory.\n",
    "\n",
    "After the service is up, get the Jupyter token:\n",
    "\n",
    "``` bash\n",
    "# run on node-data\n",
    "docker exec jupyter jupyter server list\n",
    "```\n",
    "\n",
    "Open the URL in your local browser, replacing `localhost` with your floating IP:\n",
    "\n",
    "-   `http://A.B.C.D:8888/tree?token=...`\n",
    "\n",
    "Then, in Jupyter:\n",
    "\n",
    "1.  Open `training.ipynb`.\n",
    "2.  Run cells top to bottom.\n",
    "3.  Confirm training completes successfully.\n",
    "\n",
    "The notebook uses PyIceberg to load data from the lakehouse, e.g.:\n",
    "\n",
    "``` python\n",
    "from pyiceberg.catalog import load_catalog\n",
    "\n",
    "catalog = load_catalog(\"gourmetgram\")\n",
    "table = catalog.load_table(\"moderation.training_data\")\n",
    "```\n",
    "\n",
    "Note that it does not have to specify the exact location in S3 where the data is saved - it finds out from the catalog.\n",
    "\n",
    "We don’t have a lot of data, because we haven’t been running our “service” very long! But we consider that in the future, we will have a large training data set that does not fit in memory. So, we stream Arrow batches and train incrementally:\n",
    "\n",
    "``` python\n",
    "scanner = table.scan()\n",
    "batches = scanner.to_arrow_batch_reader()\n",
    "\n",
    "for batch in batches:\n",
    "    df_batch = batch.to_pandas()\n",
    "    model.partial_fit(X_batch, y_batch, classes=[0, 1])\n",
    "```\n",
    "\n",
    "Note thatIceberg also gives us dataset versioning for training reproducibility. Every table write creates a new snapshot. Before training, we can record the snapshot id for `moderation.training_data` and save it with model artifacts. Later, we can point to the exact snapshot used for that model version.\n",
    "\n",
    "``` python\n",
    "catalog = load_catalog(\"gourmetgram\")\n",
    "table = catalog.load_table(\"moderation.training_data\")\n",
    "\n",
    "snapshot_id = table.metadata.current_snapshot_id\n",
    "print(f\"Training from Iceberg snapshot: {snapshot_id}\")\n",
    "```\n",
    "\n",
    "That means model version `N` can be traced back to a specific Iceberg snapshot, not just a vague time window."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the notebook, we save artifacts locally in the mounted workspace. Look in the `models` directory and confirm these files exist:\n",
    "\n",
    "-   `model.joblib`\n",
    "-   `scaler.joblib`\n",
    "-   `encoder.joblib`\n",
    "-   `metadata.json`\n",
    "\n",
    "Open `metadata.json` and note `snapshot_id`: this is the exact Iceberg snapshot used for the training run."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}
